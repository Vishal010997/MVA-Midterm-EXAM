---
title: "Social Media MVA"
output: html_document
date: "2024-03-29"
---


```{R}
library(readxl)
social_media <- read_excel("C:/Users/Vishal/Downloads/MVA_CLASS_COMBINE.xlsx")
str(social_media)
social_media_cleaned <- social_media[,-1]

```
```{r}
#changing column names
change_cols_index <- c(2,4,6,8,10,12,14,16,17,18,19,20,21,22,23,24)
change_cols_name <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time", "Application Type", "Interview_call_received", "Networking", "Learning", "Mood_Productivity", "Morning_tireness", "Sleep_trouble", "Weekly_Feelings")
colnames(social_media_cleaned)[change_cols_index] <- change_cols_name



social_media_cleaned

```



```{R}
# Convert "NA", "N/A", "n/a", "na", "N.A", "n.a" to 0
social_media_cleaned[social_media_cleaned == "NA" | social_media_cleaned == "N/A" | social_media_cleaned == "na" | social_media_cleaned == "n/a" | social_media_cleaned == "N.A" | social_media_cleaned == "n.a" | social_media_cleaned == "0" | social_media_cleaned == ""] <- NA
social_media_cleaned
```
```{r}
social_media_cleaned[is.na(social_media_cleaned)] <- '0'
social_media_cleaned
```
```{r}
# Define a function to convert time strings to decimal hours
convert_to_decimal_hours <- function(time_string) {
# Check if NA values are present
if (any(is.na(time_string))) {
         return(rep(NA, length(time_string)))  # Return NA for NA values
     }
     
# Define a function to convert HH:MM format to decimal hours
     hhmm_to_decimal <- function(hhmm) {
         parts <- as.numeric(strsplit(hhmm, ":")[[1]])  # Split into hours and minutes
         hours <- parts[1]
         minutes <- ifelse(length(parts) > 1, parts[2], 0)  # Handle missing minutes
         total_hours <- hours + minutes / 60
         return(total_hours)
     }
     
# Convert time strings to decimal hours
decimal_hours <- sapply(time_string, function(x) {
         if (grepl("^\\d+:\\d+$", x)) {
             return(hhmm_to_decimal(x))  # Convert HH:MM format
         } else if (grepl("^\\d+\\.\\d+$", x)) {
             return(as.numeric(x))  # Convert decimal format
         } else if (grepl("^\\d+$", x)) {
             return(as.numeric(x))  # Convert whole numbers
         } else {
             return(NA)  # Return NA for other cases
         }
     })
     
     return(decimal_hours)
}

time_columns <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time") 
# Apply the conversion function to all time columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], convert_to_decimal_hours)
 
# Verify the result
str(social_media_cleaned)

#Dropping the name columns
social_media_cleaned <- social_media_cleaned[, -c(1, 3, 5, 7, 9, 11, 13, 15)] 
social_media_cleaned
```
```{r}
# Loop through each column in time_columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], function(x) {
  # Calculate mean of the column excluding NA values
  mean_value <- mean(x, na.rm = TRUE)
  # Replace NA values with the mean
  x[is.na(x)] <- mean_value
  return(x)
})

# Print the updated data frame
print(social_media_cleaned)

```
```{r}
#Scaling the data
# Extracting the columns with names ending in "_Time"
time_columns <- grep("_Time$", names(social_media_cleaned), value = TRUE)

# Scaling the time columns
scaled_time_data <- scale(social_media_cleaned[time_columns])

#plotting the scaled values
x_time_columns <- grep("_Time$", names(social_media_cleaned), value = TRUE)
x_time_columns
```

Done With data cleaning above:

Lets start with our analysis

All the time parameters seem to be concentrated between -0.5 and 0

```{R}
# Plot histograms for each x_time column
par(mfrow = c(2, 3))

# Adjust the layout based on the number of x_time columns
for (col in x_time_columns) {
    hist(scaled_time_data, main = col, xlab = "Scaled Value")
}
```
```{r}
scaled_time_data
```

Correlation:

High Correlations:

There are strong positive correlations between time spent on various social media platforms, such as:

LinkedIn and Snapchat (0.926)

LinkedIn and YouTube (0.950)

Snapchat and YouTube (0.909)

Twitter and Reddit (0.985)

WhatsApp and LinkedIn (0.839)


Moderate Correlations:

There are moderate positive correlations between time spent on platforms like:

Twitter and YouTube (0.452)

Twitter and OTT (0.968)

Snapchat and WhatsApp (0.811)

Instagram and Twitter (0.483)


Low Correlations:

Some platforms have relatively low correlations with each other, such as:
WhatsApp and Twitter (-0.006)

WhatsApp and OTT (0.120)

Snapchat and Instagram (0.099)


Interpretation:

Strong positive correlations indicate that users who spend more time on one platform are likely to spend more time on the correlated platform as well.

Negative correlations, though rare here, indicate that as time spent on one platform increases, time spent on the other decreases, or vice versa.

Low correlations suggest that the time spent on one platform doesn't necessarily predict the time spent on another platform.
```{R}
#Performing PCA

# Compute the covariance matrix
cov_matrix <- cov(scaled_time_data)
cov_matrix
```

Eigenvalues:

They represent the variance explained by each principal component.
The first eigenvalue (4.791) is dominant, indicating the principal component with the most variance.



Eigenvectors:

They represent the direction of principal components in the original feature space.
Each column corresponds to a different principal component.




Inference:

The dominant eigenvalue suggests strong variability in the data's first principal component.
Eigenvectors provide insight into the direction of this variability.
This decomposition aids dimensionality reduction, prioritizing principal components with higher eigenvalues for retaining data variance.




```{R}
# Perform eigenvalue decomposition
eigen_result <- eigen(cov_matrix)
eigen_result
```
```{r}

# Extract eigenvalues and eigenvectors
eigenvalues <- eigen_result$values
eigenvalues
```

The first eigenvalue (4.791) is significantly larger than the others, suggesting that the corresponding principal component captures the most variance in the data.

Subsequent eigenvalues (2.357, 0.810, etc.) decrease in magnitude, indicating diminishing importance in explaining the variance.

The magnitude of each eigenvalue reflects the amount of variability accounted for by its corresponding principal component.

Prioritizing principal components based on their eigenvalues guides dimensionality reduction techniques like PCA, preserving the most significant variability while reducing dimensionality.


```{r}
eigenvectors <- eigen_result$vectors
eigenvectors
```

The number of principal components to consider is determined by counting the eigenvalues greater than 1.

In this case, there are 2 eigenvalues greater than 1, indicating that 2 principal components should be retained.

Principal components with eigenvalues above 1 explain more variance than individual variables and are thus considered significant for preserving the overall variability in the data.

Retaining these 2 principal components allows for dimensionality reduction while still capturing a substantial portion of the data's variability.

```{R}
#To find number of principal components to consider:
#1 Eigenvalue criterion
eigenvalues <- eigen_result$values
num_components <- sum(eigenvalues > 1)
print(num_components)
```

As per the Scree plot we can observe that ideally using 2 PCA's is best as 2 points are above reference line and there is a strong elbow at 2nd PCA

```{R}
#2 Scree plot
plot(eigenvalues, type = "b", main = "Scree Plot")
abline(h = 1, col = "red", lty = 2)
```

Each row corresponds to an observation, and each column represents a transformed feature.

The values in the transformed data represent the coordinates of observations in a new feature space defined by the first three principal components.

Retaining only three principal components achieves dimensionality reduction while preserving a significant portion of the original data's variance.

In essence, the transformed data simplifies the original dataset into a lower-dimensional space, facilitating analysis while retaining essential patterns.
```{R}
#Retaining 3 PC
n_components <- 3  
transformed_data <- scaled_time_data %*% eigenvectors[, 1:n_components]
print(transformed_data)
```



```{r}
library(RColorBrewer)
loadings <- eigenvectors[, 1:n_components]
my_palette <- colorRampPalette(brewer.pal(9, "YlOrRd")) 
# Visualize the loadings matrix with the specified color palette
heatmap(loadings, Rowv = NA, Colv = NA, col = my_palette(256), scale = "column", margins = c(5, 10))
```

PC2 and PC2 seem to be concentrated above 0 towards positive and some outliers at minus and one above 4 value of PC1

```{R}
# Scatter plot of the first two principal components
plot(transformed_data[, 3], transformed_data[, 1], xlab = "PC1", ylab = "PC2", 
     main = "Scatter Plot of PC1 vs PC2")

```

Below bioplot infers that Insta, OTT and Twitter are highly correlated in PC2

Rest are negatively correlated in PC2 AND all values are towards neagtive of PC1

```{R}

biplot(prcomp(scaled_time_data), scale = 0)
```


Cumulative Variance Plot":

Explains that best variance is observed till 3 PC'S
```{r}
# Cumulative Variance Plot
cumulative_variance <- cumsum(eigenvalues) / sum(eigenvalues)
plot(1:length(cumulative_variance), cumulative_variance, type = "b", 
     xlab = "Number of Principal Components", ylab = "Cumulative Variance Explained",
     main = "Cumulative Variance Plot")
```
```{R}
# 3D Scatter Plot
library(scatterplot3d)
scatterplot3d(transformed_data[, 1], transformed_data[, 2], transformed_data[, 3],
              xlab = "PC1", ylab = "PC2", zlab = "PC3",
              main = "3D Scatter Plot of PC1, PC2, and PC3")
```

LinkedIn and Snapchat:

There is a strong positive correlation between the time spent on LinkedIn and Snapchat.
Users who spend more time on LinkedIn are likely to spend a considerable amount of time on Snapchat, and vice versa.
LinkedIn and YouTube:

A significant positive correlation exists between the time spent on LinkedIn and YouTube.
Users who engage more with LinkedIn content are also likely to spend substantial time on YouTube, and vice versa.
Snapchat and YouTube:

There is a strong positive correlation between the time spent on Snapchat and YouTube.
Users who use Snapchat frequently are also likely to spend a significant amount of time on YouTube, and vice versa.
Twitter and Reddit:

A very strong positive correlation exists between the time spent on Twitter and Reddit.
Users who are active on Twitter tend to be similarly active on Reddit, and vice versa.
WhatsApp and LinkedIn:

There is a notable positive correlation between the time spent on WhatsApp and LinkedIn.
Users who dedicate more time to WhatsApp messaging also tend to spend considerable time on LinkedIn, and vice versa.

```{R}
# Heatmap of Correlation Matrix
corr_matrix <- cor(scaled_time_data)
heatmap(corr_matrix, Rowv = NA, Colv = NA, col = heat.colors(256))
```


```{R}
#Cluster Analysis

# Define the maximum number of clusters to test
max_k <- 10

# Perform K-means clustering for different values of k
wcss <- numeric(length = max_k)
for (i in 1:max_k) {
  kmeans_model <- kmeans(scaled_time_data, centers = i)
  wcss[i] <- kmeans_model$tot.withinss
}
```

As per the elbow plot Best output for k-means cluster is yielded at 3-4 clusters as elbow is present at that point

Sum of square used to check the performance
```{R}
# Plot the elbow method
plot(1:max_k, wcss, type = "b", xlab = "Number of clusters (k)", ylab = "Within-cluster sum of squares (WCSS)")
```
```{r}


# Convert scaled_time_data to numeric
scaled_time_data <- apply(scaled_time_data, 2, as.numeric)

# Assuming your data is stored in a matrix named 'scaled_time_data'
# Use max.col to get the index of the maximum value in each column
max_indices <- max.col(scaled_time_data, ties.method = "first")

# Create a vector to store the column names corresponding to the maximum values
max_column <- colnames(scaled_time_data)[max_indices]

# Add the max_column vector as a new column to the data matrix
social_media_data_with_max <- cbind(scaled_time_data, max_column)

# Verify the conversion
str(social_media_data_with_max)
social_media_data_with_max
```
```{r}
# Assuming your data is stored in a matrix named 'social_media_data_with_max'

# Get the names of all columns
all_columns <- colnames(social_media_data_with_max)

# Convert all columns to numeric except max_column
for (col in names(social_media_data_with_max)) {
  if (col != "max_column") {
    social_media_data_with_max[, col] <- as.numeric(social_media_data_with_max[, col])
  }
}

# Verify the conversion
str(social_media_data_with_max)
social_media_data_with_max
```
Cluster Centers: The cluster centers represent the average values of each variable (Instagram_Time, Linkedin_Time, Snapchat_Time, Twitter_Time, Whatsapp_Time, Youtube_Time, OTT_Time, Reddit_Time) within each cluster. These values indicate the typical behavior or usage pattern of each cluster regarding different social media platforms.

Cluster 1: Moderate usage across most platforms, with slightly higher usage on Instagram, LinkedIn, and Snapchat.
Cluster 2: Low usage across all platforms except for moderate usage on Reddit.
Cluster 3: Moderate to high usage on Twitter and Snapchat, with relatively low usage on other platforms.
Cluster 4: Very high usage on LinkedIn, Snapchat, and Whatsapp, with low usage on other platforms.
Cluster 5: Very high usage on Instagram and Reddit, moderate usage on other platforms.
Cluster 6: Very high usage on OTT platforms, moderate usage on other platforms.




```{R}
# Step 1: Choose the number of clusters (K)
K <- 6 
# Step 2: Perform K-means clustering
kmeans_result <- kmeans(scaled_time_data, centers = K)
# Step 3: View cluster centers
print(kmeans_result$centers)
# Step 4: View cluster assignments for each data point
print(kmeans_result$cluster)
```

Clustering Vector: This vector indicates the cluster assignment for each data point (row) in the dataset. In this analysis, data points are assigned to either Cluster 1 or Cluster 2 based on their similarity to the cluster means.

In the given example, the clustering vector indicates that the first data point is assigned to Cluster 2, while the remaining data points are assigned to Cluster 1.
Percentage of Variation Accounted For: This metric measures how well the clustering algorithm explains the variance in the data. It is calculated as the percentage reduction in within-cluster sum of squares (WSS) relative to the total sum of squares (TSS).

In this case, the percentage of variation accounted for by the two clusters is computed as 82.5%. This suggests that the clustering algorithm effectively captured the underlying patterns in the data, explaining a substantial portion of the variance.

```{r}
# Computing the percentage of variation accounted for. Two clusters
(kmeans2.sm <- kmeans(scaled_time_data,2,nstart = 10))
perc.var.2 <- round(100*(1 - kmeans2.sm$betweenss/kmeans2.sm$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2
```

Cluster 1: Exceptional engagement across all platforms.
Cluster 2: Majority with lower engagement.
Cluster 3: Moderate engagement, mainly on Instagram, LinkedIn, and Twitter.
The analysis captures 16.4% of the total variance.
```{R}
# Computing the percentage of variation accounted for. Three clusters
(kmeans3.sm <- kmeans(scaled_time_data,3,nstart = 10))
perc.var.3 <- round(100*(1 - kmeans3.sm$betweenss/kmeans2.sm$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3
```
```{r}
# Computing the percentage of variation accounted for. Four clusters
(kmeans4.sm <- kmeans(scaled_time_data,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.sm$betweenss/kmeans4.sm$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4
```
```{r}
# Computing the percentage of variation accounted for. Five clusters
(kmeans5.sm <- kmeans(scaled_time_data,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.sm$betweenss/kmeans5.sm$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5
```
```{R}
# Computing the percentage of variation accounted for. Six clusters
(kmeans6.sm <- kmeans(scaled_time_data,6,nstart = 10))
perc.var.6 <- round(100*(1 - kmeans6.sm$betweenss/kmeans6.sm$totss),1)
names(perc.var.6) <- "Perc. 6 clus"
perc.var.6
```
```{R}
attributes(perc.var.6)
```
46.6% of the variance for 2 clusters.
16.4% for 3 clusters.
3.5% for 4 clusters.
2.5% for 5 clusters.
1.9% for 6 clusters.


A two-cluster solution captures the highest variance at 46.6%, suggesting a meaningful partition of the data. As the number of clusters increases, the additional explained variance diminishes, indicating diminishing returns beyond two clusters.

```{r}
Variance_List <- c(perc.var.2,perc.var.3,perc.var.4,perc.var.5,perc.var.6)

Variance_List
plot(Variance_List)
```

As per the below elbow plot 4 cluster centroid are the best due to strong elbow
```{R}
#elbow method
# Perform K-means clustering for different numbers of clusters
wss <- numeric(length = 5)

for (k in 2:6) {
  kmeans_model <- kmeans(scaled_time_data, k, nstart = 10)
  wss[k - 1] <- kmeans_model$tot.withinss
}

#elbow curve
plot(2:6, wss, type = "b", xlab = "Number of Clusters", ylab = "Within-cluster Sum of Squares", main = "Elbow Method")
```
```{r}
#silhouette method

library(cluster)

sil_width <- numeric(length = 5)

# Computing silhouette widths for different numbers of clusters
for (k in 2:6) {
  # Perform K-means clustering
  kmeans_model <- kmeans(scaled_time_data, k, nstart = 10)
  
  if (k > 1) {  # Silhouette width using two clusters
    silhouette_width <- silhouette(kmeans_model$cluster, dist(scaled_time_data))
    sil_width[k - 1] <- mean(silhouette_width[, "sil_width"])
  }
}

# Plot the silhouette scores
plot(2:6, sil_width, type = "b")
```



The cluster membership indicates that each data point belongs to one of five distinct groups, with the majority of data points being assigned to clusters 2 and 4. This distribution suggests that there are dominant patterns or behaviors shared among a significant portion of the data, while the remaining clusters represent smaller, possibly more specialized segments. 

Further analysis of the characteristics of each cluster can provide insights into the underlying patterns present in the data and guide targeted decision-making or intervention strategies tailored to the needs of each group.
```{r}
#2. Show the membership for each cluster
optimal_num_clusters <- 5

#K-means clustering with the optimal number of clusters
kmeans_model <- kmeans(scaled_time_data, optimal_num_clusters, nstart = 10)

cluster_membership <- kmeans_model$cluster

# Print the cluster membership for each data point
print(cluster_membership)
```

As per below observations:

Clusters indicate Cluster 2 and 4 seem to lie towards left bottom

```{R}
# Scatter plot of the data with clusters colored by membership
plot(scaled_time_data[, 1], scaled_time_data[, 2], 
     col = kmeans_model$cluster, pch = 16, 
     xlab = "Variable 1", ylab = "Variable 2",
     main = "K-means Clustering")

# Adding cluster centers to the plot
points(kmeans_model$centers[, 1], kmeans_model$centers[, 2], col = 1:optimal_num_clusters, pch = 3, cex = 2)

# Adding legend
legend("topright", legend = paste("Cluster", 1:optimal_num_clusters), col = 1:optimal_num_clusters, pch = 16, cex = 1.2, title = "Clusters")
```
```{R}
#3. Show a visualization of the cluster and membership using the first two Principal Components.

# Perform PCA
pca_result <- prcomp(scaled_time_data, scale. = TRUE)

# Extract PC scores for the first three principal components
PC1 <- pca_result$x[, 1]
PC2 <- pca_result$x[, 2]
PC3 <- pca_result$x[, 3]

# Plot the data with clusters colored by membership
plot(PC1, PC2, 
     col = kmeans_model$cluster, pch = 16, 
     xlab = "Principal Component 1", ylab = "Principal Component 2",
     main = "K-means Clustering based on PC1 and PC2")

# Add legend
legend("topright", legend = paste("Cluster", 1:optimal_num_clusters), 
       col = 1:optimal_num_clusters, pch = 16, cex = 1.2, title = "Clusters")
```


Cluster 1 (clus1): Low engagement across all platforms, indicating minimal or sporadic activity.

Cluster 2 (clus2): Below-average engagement on various platforms, with values mostly scaled around -0.3 to -0.5.

Cluster 3 (clus3): High engagement across all platforms, particularly on Instagram, Linkedin, Snapchat, and Twitter.

Cluster 4 (clus4): High engagement, especially on Linkedin, suggesting professional or business-focused users.

Cluster 5 (clus5): Mixed engagement, with exceptionally high activity on Instagram and relatively lower engagement on other platforms.
```{r}
# Saving five k-means clusters in a list
# Initialize a list to store matrices for each cluster
cluster_matrices <- list()
 
# Iterate over each cluster
for (i in 1:max(kmeans5.sm$cluster)) {
     # Subset data for the current cluster
     cluster_indices <- which(kmeans5.sm$cluster == i)
     cluster_data <- scaled_time_data[cluster_indices, ]
     
     # Store the cluster data in the list
     cluster_matrices[[paste0("clus", i)]] <- cluster_data
 }
 
# Check the list of matrices
print(cluster_matrices)
```

```{R}
#Factor Analysis
library(psych)

fit.pc_sm <- principal(scaled_time_data, nfactors=4, rotate="varimax")
fit.pc_sm
```
```{r}
# Extract eigenvalues
eigenvalues <- fit.pc_sm$values
# Create scree plot
plot(1:length(eigenvalues), eigenvalues, type="b", 
      xlab="Component Number", ylab="Eigenvalues", main="Scree Plot")
# Add a horizontal line at y = 1 for reference (Kaiser's criterion)
abline(h=1, col="red", lty=2)
# Add text label for Kaiser's criterion
text(length(eigenvalues), 1, "Kaiser's Criterion (Eigenvalue = 1)", pos=4, col="red")

round(fit.pc_sm$values, 3)
fit.pc_sm$loadings
```
```{r}
# Loadings with more digits
for (i in c(1,2,3,4)) { print(fit.pc_sm$loadings[[1,i]])}
# Communalities
fit.pc_sm$communality
# Rotated factor scores, Notice the columns ordering: RC1, RC3, RC2 and RC4
fit.pc_sm$scores
```
```{r}
#FA model
fa.plot(fit.pc_sm)
```
```{R}
fa.diagram(fit.pc_sm)
```
```{r}
vss(scaled_time_data)
```
```{R}
# Computing Correlation Matrix
corrm.sm <- cor(scaled_time_data)
corrm.sm
plot(corrm.sm)
```
```{R}
sm_pca <- prcomp(scaled_time_data, scale=TRUE)
summary(sm_pca)
plot(sm_pca)

# A table containing eigenvalues and %'s accounted, follows. Eigenvalues are the sdev^2
(eigen_sm <- round(sm_pca$sdev^2,3))
round(fit.pc_sm$values, 3)
names(eigen_sm) <- paste("PC",1:8,sep="")
eigen_sm

sumlambdas <- sum(eigen_sm)
sumlambdas

propvar <- round(eigen_sm/sumlambdas,2)
propvar

cumvar_sm <- cumsum(propvar)
cumvar_sm

matlambdas <- rbind(eigen_sm,propvar,cumvar_sm)
matlambdas

rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
rownames(matlambdas)

eigvec.sm <- sm_pca$rotation
print(sm_pca)

# Taking the first two PCs to generate linear combinations for all the variables with two factors
pcafactors.sm <- eigvec.sm[,1:2]
pcafactors.sm

# Multiplying each column of the eigenvectorâ€™s matrix by the square-root of the corresponding eigenvalue in order to get the factor loadings
unrot.fact.sm <- sweep(pcafactors.sm,MARGIN=2,sm_pca$sdev[1:2],`*`)
unrot.fact.sm

# Computing communalities
communalities.sm <- rowSums(unrot.fact.sm^2)
communalities.sm

# Performing the varimax rotation. The default in the varimax function is norm=TRUE thus, Kaiser normalization is carried out
rot.fact.sm <- varimax(unrot.fact.sm)
#View(unrot.fact.fish)
rot.fact.sm

# The print method of varimax omits loadings less than abs(0.1). In order to display all the loadings, it is necessary to ask explicitly the contents of the object $loadings
fact.load.sm <- rot.fact.sm$loadings[1:8,1:2]
fact.load.sm

as.matrix(scaled_time_data)%*%fact.load.sm%*%solve(t(fact.load.sm)%*%fact.load.sm)

# Show the columns (variables) that go into each factor
for (i in 1:ncol(fact.load.sm)) {
  cat("Variables for Factor PC", i, ":\n")
  print(rownames(fact.load.sm)[order(abs(fact.load.sm[, i]), decreasing = TRUE)])
  cat("\n")
}


# Convert factor loadings matrix to a data frame
fact_loadings_df <- as.data.frame(fact.load.sm)
# Add a column for variable names
fact_loadings_df$Variable <- rownames(fact_loadings_df)
# Reshape data for plotting
fact_loadings_df_long <- reshape2::melt(fact_loadings_df, id.vars = "Variable", variable.name = "Factor")

library(ggplot2)
#HeatMap
ggplot(fact_loadings_df_long, aes(x = Factor, y = Variable, fill = value)) +
     geom_tile() +
     scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
     labs(x = "Factor", y = "Variable", title = "Factor Loadings Heatmap") +
     theme_minimal() +
     theme(axis.text.x = element_text(angle = 45, hjust = 1))

#BarPlot
ggplot(fact_loadings_df_long, aes(x = Factor, y = value, fill = Variable)) +
     geom_bar(stat = "identity", position = "dodge") +
     labs(x = "Factor", y = "Factor Loading", fill = "Variable", title = "Factor Loadings Bar Plot") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1),
           legend.position = "right")

```

Factor Analysis Insights:

The factor loadings depict the correlation between each social media platform and the underlying factors. Higher absolute loadings indicate a stronger association between the platform and the factor.
Factors with loadings surpassing a certain threshold (e.g., 0.3 or 0.4) may be considered significant contributors to the respective factors.
PCA Insights:

PCA identifies patterns and relationships among the social media platforms, condensing the information into principal components.
Each principal component captures a portion of the total variance in the data, with higher proportions indicating greater importance in explaining the variability.
The transformed data represents the original data projected onto the principal components, facilitating dimensionality reduction while retaining most of the variance.
Comparison with the Class:

Comparing individual factor loadings or principal component scores with those of the class can unveil similarities or discrepancies in social media usage patterns.
Similarities across factors/components may imply common trends or preferences within the class, while differences could signify unique behaviors or preferences.
Interpretation of Clustering Results:

Clustering outcomes offer insights into how individuals or groups of individuals (clusters) exhibit similar behaviors across various social media platforms.
Understanding cluster characteristics can inform targeted marketing strategies, personalized recommendations, or segmentation strategies based on social media usage patterns.